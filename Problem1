
sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" \
--username retail_dba \
--password itversity \
--table orders \
--target-dir /user/phani111/problem1/orders \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec

sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" \
--username retail_dba \
--password itversity \
--table order_items \
--target-dir /user/phani111/problem1/order-items \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec





spark-shell --packages com.databricks:spark-avro_2.10:2.0.1  --master yarn --conf spark.ui.port=56787 

val sqlContext = new org.apache.spark.sql.SQLContext(sc)


import com.databricks.spark.avro._

import sqlContext.implicits._

val OrdersDF = sqlContext.read.avro("/user/phani111/problem1/orders")

val orderItemsDF = sqlContext.read.avro("/user/phani111/problem1/order-items")

val OrdersJoinedDF = OrdersDF.join(orderItemsDF, OrdersDF("order_id") === orderItemsDF("order_item_order_id"))


val dataFrameResult = OrdersJoinedDF.groupBy(to_date(from_unixtime(col("order_date")/1000)).alias("Order_Date_formatted"),col("order_status")).agg(round(sum(col("order_item_subtotal")),2).alias("total_amount"),countDistinct(col("order_id")).alias("total_orders")).orderBy(col("Order_Date_formatted").desc,col("order_status"),col("total_amount").desc,col("total_orders"))


OrdersJoinedDF.registerTempTable("order_joined")
 
val sqlresult = sqlContext.sql("select to_date(from_unixtime(cast(order_date/1000 as bigint))) as Order_Date_formatted, order_status, cast(sum(order_item_subtotal) as DECIMAL(10,2)) as total_amount, count(distinct order_id) as total_orders from order_joined group by order_date,order_status order by Order_Date_formatted desc, order_status,total_amount desc, total_orders ")

sqlContext.sql("select * from order_joined limit 1")




val OrdersJoinedDF1 = OrdersJoinedDF.map(x=> ((x(1).toString,x(3).toString),(x(8).toString.toFloat,1)))

val OrdersJoinedDF3 = OrdersJoinedDF1.reduceByKey((total, elem) => (total._1+elem._1,total._2+elem._2))

OrdersJoinedDF3.take(10).foreach(println)

OrderJoinedDF2.take(5).foreach(println)
val OrdersJoinedDF1 = OrdersJoinedDF.map(x=> ((x(1).toString,x(3).toString),(x(8).toString.toFloat))


val OrderJoinedDF2 = OrdersJoinedDF1.aggregateByKey((0.0,0))((total, elem) =>(total._1 + elem, total._2+1),(final, interm) => (final._1+interm._1, final._2+interm._2))


var comByKeyResult  = OrdersJoinedDF.map(x=> ((x(1).toString,x(3).toString),(x(8).toString.toFloat,x(0).toString)))

var y = Set[String]()

comByKeyResult.aggregateByKey((0.0,y))((total,value) => (total._1+value._1,total._2 + value._2),(xtotal,ytotal)=>(xtotal._1+ytotal._1,xtotal._2++ytotal._2))

val comByKeyResult1 = comByKeyResult.aggregateByKey((0.0,y))((total,value) => (total._1+value._1,total._2 + value._2),(xtotal,ytotal)=>(xtotal._1+ytotal._1,xtotal._2++ytotal._2)).map(x => (x._1._1,x._1._2,x._2._1,x._2._2.size)).toDF().orderBy(col("_1").desc,col("_2"),col("_3").desc,col("_4"))


sqlContext.setConf("spark.sql.parquet.compression.codec","snappy")


sqlContext.setConf("spark.sql.parquet.compression.codec","gzip")

comByKeyResult1.show()

